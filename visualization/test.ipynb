{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hqyone/PycharmProjects/data_output/tRNA_seq/output/static.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_loader as dl\n",
    "import correlation_plot as cp\n",
    "import profile_plot as dp\n",
    "import express_statistic_plot as es\n",
    "import trf_type_barplot as tt\n",
    "import read_statistic as rs\n",
    "import pathlib\n",
    "import pylab\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "'''\n",
    "# Visualization module of tRNAExploer (v1.0) \n",
    "# The visual configure file\n",
    "'''\n",
    "#visual_config = \"/Users/hqyone/PycharmProjects/data_output/server2/visual_config.tsv\"\n",
    "visual_config = \"/Users/hqyone/PycharmProjects/data_output/tRNA_seq/output/visual_config.tsv\"\n",
    "#visual_config = \"/Users/hqyone/PycharmProjects/data_output/cancer/output/visual_config.tsv\"\n",
    "wdir = '/media/hqyone/3T/trna/data/trna/trna_test_out'\n",
    "visual_config=wdir+'/visual_config.tsv'\n",
    "\n",
    "if not os.path.isfile(visual_config):\n",
    "    print(\"Can't find config file. Abort !\")\n",
    "    exit(-1)\n",
    "\n",
    "# If you don't set report_dir, the default path will be <output_dir>+\"/reports\"\n",
    "report_dir = \"\"\n",
    "\n",
    "# Loading data\n",
    "d =  dl.LoadConfig(visual_config, report_dir)\n",
    "\n",
    "def csv_download_link(df, csv_file_name, delete_prompt=True):\n",
    "    \"\"\"Display a download link to load a data frame as csv from within a Jupyter notebook\"\"\"\n",
    "    df.to_csv(csv_file_name, index=True, sep='\\t')\n",
    "    from IPython.display import FileLink\n",
    "    display(FileLink(csv_file_name))\n",
    "    if delete_prompt:\n",
    "        a = input('Press enter to delete the file after you have downloaded it.')\n",
    "        import os\n",
    "        os.remove(csv_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# The function is used to selected o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        SRR1836129\n",
      "1        SRR1836129\n",
      "2        SRR1836129\n",
      "3        SRR1836129\n",
      "4        SRR1836129\n",
      "            ...    \n",
      "16560    SRR1836127\n",
      "16561    SRR1836127\n",
      "16562    SRR1836127\n",
      "16563    SRR1836127\n",
      "16564    SRR1836127\n",
      "Name: #SampleID, Length: 16565, dtype: object\n",
      "Index(['#chrom', 'start', 'end', 'name', 'score', 'strand', 'full_seq', 'seq',\n",
      "       'intron_infor', 'anticodon', 'anticodon_start', 'anticodon_end',\n",
      "       'acceptor', 'family', 'seq_utr', 'utr_len', 'map_start', 'map_end',\n",
      "       'map_len', 'map_structure_str', 'map_seq', 'anticodon_start_in_map',\n",
      "       'anticodon_end_in_map', 'map_scan_score', 'possible_type',\n",
      "       'd_loop_start', 'd_loop_lstart', 'd_loop_lend', 'd_loop_end',\n",
      "       'a_loop_start', 'a_loop_lstart', 'a_loop_lend', 'a_loop_end',\n",
      "       'v_loop_start', 'v_loop_lstart', 'v_loop_lend', 'v_loop_end',\n",
      "       't_loop_start', 't_loop_lstart', 't_loop_lend', 't_loop_end',\n",
      "       'stem_for_start', 'stem_for_end', 'stem_rev_start', 'stem_rev_end'],\n",
      "      dtype='object')\n",
      "intron: 39-61 (99-121)\n",
      "intron: 39-61 (99-121)\n",
      "[0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 1.6680e+03 1.9740e+03 1.9740e+03 2.6460e+03 2.6460e+03 3.2300e+03\n",
      " 9.2200e+03 1.1206e+04 1.1366e+04 1.1444e+04 1.3262e+04 1.4520e+04\n",
      " 1.4734e+04 1.5944e+04 1.6156e+04 1.6390e+04 1.6434e+04 1.6542e+04\n",
      " 1.7360e+04 1.7480e+04 1.8514e+04 2.1082e+04 2.1924e+04 2.2010e+04\n",
      " 2.2144e+04 2.2144e+04 2.2144e+04 3.4678e+04 3.5054e+04 3.5256e+04\n",
      " 3.5256e+04 3.5424e+04 3.5526e+04 3.5860e+04 3.6406e+04 4.5124e+04\n",
      " 4.8220e+04 4.8276e+04 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04\n",
      " 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04\n",
      " 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04 4.0887e+04\n",
      " 4.0887e+04 4.0875e+04 4.0875e+04 4.0875e+04 4.0875e+04 4.0937e+04\n",
      " 4.0937e+04 8.2981e+04 8.3391e+04 8.3603e+04 8.3935e+04 8.4273e+04\n",
      " 8.4859e+04 8.8987e+04 9.0859e+04 9.2312e+04 9.3535e+04 9.4667e+04\n",
      " 9.5631e+04 9.5889e+04 9.6473e+04 9.6753e+04 9.7253e+04 9.8011e+04\n",
      " 9.8455e+04 9.9301e+04 9.9527e+04 9.9569e+04 9.9595e+04 9.9595e+04\n",
      " 9.9564e+04 9.9533e+04 9.9543e+04 9.9635e+04 9.9635e+04 9.9651e+04\n",
      " 9.9661e+04 9.9737e+04 9.9737e+04 9.9737e+04 9.9737e+04 9.9737e+04\n",
      " 9.9737e+04 9.9737e+04 9.9737e+04 9.9737e+04 9.9737e+04 9.9737e+04\n",
      " 9.9737e+04 9.9737e+04 9.9737e+04 9.9035e+04 1.2200e+02 1.2200e+02\n",
      " 1.2200e+02 1.2200e+02 1.2200e+02 1.2200e+02 1.2200e+02 1.2200e+02\n",
      " 1.2200e+02 1.1100e+02 1.0000e+02 1.0000e+02 1.0000e+02 1.0000e+02\n",
      " 1.0000e+02 1.0000e+02 1.0000e+02 1.0000e+02 1.0000e+02 1.0000e+02\n",
      " 1.0000e+02 1.0000e+02 5.0000e+01 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.8300e+02\n",
      " 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02\n",
      " 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02\n",
      " 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02\n",
      " 4.1600e+02 4.1600e+02 4.1600e+02 4.1600e+02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def CombineProfilesWithIntron(sample , tRNA1_ID, tRNA2_ID, d, profile_type):\n",
    "    p_df = pd.read_csv(d[\"profiles\"], sep=\"\\t\")\n",
    "    st_df = d[\"st_df\"]\n",
    "    print(st_df.columns)\n",
    "    t1_df = p_df[(p_df['tRNA_ID'] == tRNA1_ID) & (p_df['#SampleID'] == sample) & (p_df['type'] == profile_type)]\n",
    "    t2_df = p_df[(p_df['tRNA_ID'] == tRNA2_ID) & (p_df['#SampleID'] == sample) & (p_df['type'] == profile_type)]\n",
    "    profile_1 = np.array(t1_df['profile'].values[0].split(\",\")).astype(np.float)\n",
    "    profile_2 = np.array(t2_df['profile'].values[0].split(\",\")).astype(np.float)\n",
    "    \n",
    "    intron_1_str = st_df[st_df['name']==tRNA1_ID]['intron_infor'].values[0]\n",
    "    print(intron_1_str)\n",
    "    intron_1_obj = {}\n",
    "    m = re.search(r'\\((\\d+)-(\\d+)\\)', intron_1_str)\n",
    "    if m:\n",
    "        intron_1_obj['start'] = int(m.group(1))\n",
    "        intron_1_obj['end'] = int(m.group(2))\n",
    "    print(intron_1_str)\n",
    "    intron_2_str = st_df[st_df['name']==tRNA2_ID]['intron_infor'].values[0]\n",
    "    intron_2_obj = {}\n",
    "    m = re.search(r'\\((\\d+)-(\\d+)\\)', intron_2_str)\n",
    "    if m:\n",
    "        intron_2_obj['start'] = int(m.group(1))\n",
    "        intron_2_obj['end'] = int(m.group(2))\n",
    "    \n",
    "    index1=0\n",
    "    index2=0\n",
    "    for i in range(0,len(profile_1)):\n",
    "        if index1==intron_1_obj['start']-1:\n",
    "            index1=intron_1_obj['end']\n",
    "        if index2==intron_2_obj['start']-1:\n",
    "            index2=intron_2_obj['end']\n",
    "        if index1<len(profile_1) and index2<len(profile_2):\n",
    "            profile_1[index1]+= profile_2[index2]\n",
    "        index1+=1\n",
    "        index2+=1\n",
    "    return profile_1\n",
    "\n",
    "p_df = pd.read_csv(d[\"profiles\"], sep=\"\\t\")\n",
    "sample_ls = p_df[\"#SampleID\"]\n",
    "print(sample_ls)\n",
    "print(CombineProfilesWithIntron('SRR1836129' , 'tRNA-Leu-CAA-1-1', 'tRNA-Leu-CAA-1-2', d, 'total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import clusterMatrix as cm\n",
    "\n",
    "sns.set()\n",
    "\n",
    "var = d['variants']\n",
    "\n",
    "sample_ls = var[\"#SampleID\"].unique()\n",
    "sample_ls.sort()\n",
    "print(sample_ls)\n",
    "\n",
    "#Calulate the percentage of mut reads for total trf type.\n",
    "proj_name = 'test'\n",
    "\n",
    "result_df =pd.DataFrame(columns=['sample','s_desc','trf_type','mut_reads','no_mut_reads','total_reads','ratio'])\n",
    "for s in sample_ls:\n",
    "    trf_file = wdir+'/'+s+'_'+proj_name+\"_hit.tab\"\n",
    "    s_desc = d[\"sample_dic\"][s]\n",
    "    TRF = pd.read_csv(trf_file, sep=\"\\t\", index_col=False)\n",
    "    dic = {s:{'sample':s, 's_desc':s_desc,'data':{}}}\n",
    "    for index, row in TRF.iterrows():\n",
    "        read_num = int(row['mean_number'])\n",
    "        trf_type = row['TRF_type']\n",
    "        infor_contents  =row['brief_mapping_infor'].split(',')\n",
    "        if not 'total' in dic[s]['data']:\n",
    "            dic[s]['data']['total']={'mut_reads':0, 'no_mut_reads':0, 'total_reads':0, 'ratio':'0'}\n",
    "        if not trf_type in dic[s]['data']:\n",
    "            dic[s]['data'][trf_type]={'mut_reads':0, 'no_mut_reads':0, 'total_reads':0, 'ratio':'0'}\n",
    "        dic[s]['data']['total']['total_reads']+=read_num\n",
    "        dic[s]['data'][trf_type]['total_reads']+=read_num\n",
    "        if infor_contents[4]==infor_contents[5]:\n",
    "            dic[s]['data']['total']['no_mut_reads']+=read_num\n",
    "            dic[s]['data'][trf_type]['no_mut_reads']+=read_num\n",
    "        else:\n",
    "            dic[s]['data']['total']['mut_reads']+=read_num\n",
    "            dic[s]['data'][trf_type]['mut_reads']+=read_num\n",
    "    for t in dic[s]['data']:\n",
    "        row = {\n",
    "            'sample':dic[s]['sample'],\n",
    "            's_desc':dic[s]['s_desc'],\n",
    "            'trf_type':t,\n",
    "            'mut_reads':dic[s]['data'][t]['mut_reads'],\n",
    "            'no_mut_reads':dic[s]['data'][t]['no_mut_reads'],\n",
    "            'total_reads':dic[s]['data'][t]['total_reads'],\n",
    "            'ratio':round(dic[s]['data'][t]['mut_reads']/dic[s]['data'][t]['total_reads'],3)\n",
    "        }\n",
    "        result_df = result_df.append(row,ignore_index=True)\n",
    "\n",
    "csv_download_link(result_df, 'var_stat', False)\n",
    "            \n",
    "#Calulate the matrix of mutation across samples.\n",
    "mut_read_cutoff = 200\n",
    "mut_ratio_cutoff =0.8\n",
    "\n",
    "var = var.loc[var['mut_reads']>mut_read_cutoff]\n",
    "#sns.distplot(var['ratio'])\n",
    "var_group= var.groupby(['#SampleID', 'family','loc','ref']).sum()\n",
    "var_c_group= var.groupby(['#SampleID', 'family','loc','ref']).count()\n",
    "var_group['ratio_10']= var_group['mut_reads']/var_group['total_reads']\n",
    "sns.distplot(var_group['ratio_10'])\n",
    "\n",
    "#var_group['mut_id']= var_group['family']+var_group['loc'].map(str)\n",
    "var_group['mut_number'] = var_c_group['ratio']\n",
    "var_10 = var_group.loc[var_group['ratio_10']>mut_ratio_cutoff]\n",
    "#print(var_10)\n",
    "print(var_10.xs('SRR1836123',level='#SampleID', drop_level=False))\n",
    "mut_dic = {}\n",
    "\n",
    "\n",
    "sample_ls=['SRR1836123',  'SRR1836124','SRR1836125',  'SRR1836126']\n",
    "#sample_ls=['SRR1836125',  'SRR1836126']\n",
    "#sample_ls=['SRR1836127',  'SRR1836128',  'SRR1836129',  'SRR1836130']\n",
    "\n",
    "mut_matrix = pd.DataFrame(None, columns=sample_ls)\n",
    "mut_ls = []\n",
    "for index, row in var_10.iterrows():\n",
    "    s_id = index[0]\n",
    "    if not s_id in mut_dic:\n",
    "        mut_dic[s_id]={}\n",
    "    mut_id = index[1]+\":\"+str(index[2])+\":\"+index[3]\n",
    "    if mut_id not in mut_ls:\n",
    "        mut_ls.append(mut_id)\n",
    "    if not mut_id in mut_dic[s_id]:\n",
    "        mut_dic[s_id][mut_id]={}\n",
    "    mut_dic[s_id][mut_id]={'r':row['ratio_10'],'n':row['mut_number']}\n",
    "\n",
    "#Fill the matrix\n",
    "#print(mut_ls)\n",
    "sel_mut_ls = []\n",
    "for m in mut_ls:\n",
    "    row={}\n",
    "    for s in sample_ls:\n",
    "        if not m in mut_dic[s]:\n",
    "            row[s]=0\n",
    "        else:\n",
    "            row[s]=mut_dic[s][m]['r']\n",
    "    '''\n",
    "    if ((row[sample_ls[0]]>mut_ratio_cutoff and row[sample_ls[1]]>mut_ratio_cutoff) or \n",
    "        (row[sample_ls[2]]>mut_ratio_cutoff and row[sample_ls[3]]>mut_ratio_cutoff)):\n",
    "        mut_matrix=mut_matrix.append(row, ignore_index=True)\n",
    "        sel_mut_ls.append(m)\n",
    "    '''\n",
    "    mut_matrix=mut_matrix.append(row, ignore_index=True)\n",
    "    sel_mut_ls.append(m)\n",
    "mut_matrix['mut']=sel_mut_ls\n",
    "mut_matrix = mut_matrix.set_index('mut',drop=True, append=False, inplace=False, verify_integrity=False)\n",
    "#print(mut_matrix)\n",
    "    \n",
    "\n",
    "csv_download_link(mut_matrix, 'var_group', False)\n",
    "\n",
    "figure = d[\"report_dir\"]+\"var_matrix.png\"\n",
    "cm.drawExpressionClusterMatrix(mut_matrix, \n",
    "                               figure,\n",
    "                               yaxis_font_size=7,\n",
    "                               fig_width=8, \n",
    "                               fig_height=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /media/hqyone/3T/trna/data/celllines/tap/SRR1836123_test_hit.tab does not exist: '/media/hqyone/3T/trna/data/celllines/tap/SRR1836123_test_hit.tab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5aae579be29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_des\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdict1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_test_hit.tab\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mhit_df\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_aa_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrna_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tRNA_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# .str.contains('Ju')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /media/hqyone/3T/trna/data/celllines/tap/SRR1836123_test_hit.tab does not exist: '/media/hqyone/3T/trna/data/celllines/tap/SRR1836123_test_hit.tab'"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "\n",
    "wdir = '/media/hqyone/3T/trna/data/celllines/tap'\n",
    "dict1 = OrderedDict(sorted(d[\"sample_dic\"].items())) \n",
    "# print(dict1)\n",
    "\n",
    "for s, s_des in dict1.items():\n",
    "    tab_file=wdir+\"/\"+s+\"_test_hit.tab\"\n",
    "    df = pd.read_csv(tab_file, sep=\"\\t\", index_col=False).fillna('')\n",
    "    hit_df= dl.add_aa_column(df, trna_id=\"tRNA_id\")\n",
    "    # .str.contains('Ju')\n",
    "    aa_hit_df = hit_df[hit_df['read_3_fragment'].str.contains('AAAAAAA')]\n",
    "    cca_aa_hit_df = aa_hit_df[aa_hit_df['read_fragment'].str.endswith('CCA')]\n",
    "    #print(aa_hit_df['read_3_fragment'])\n",
    "    cca_aa_reads = cca_aa_hit_df['mean_number'].sum()\n",
    "    aa_reads = aa_hit_df['mean_number'].sum()\n",
    "    noaa_hit_df = hit_df[~hit_df['read_3_fragment'].str.contains('AAAAAAA')]\n",
    "    noaa_reads = noaa_hit_df['mean_number'].sum()\n",
    "    print(\"######\"+s_des)\n",
    "    print(str(int(cca_aa_reads))+\":\"+str(int(aa_reads))+\":\"+str(int(noaa_reads))+\":\"+str(int(aa_reads+noaa_reads)))\n",
    "    print(s_des+\":\"+str(round(aa_reads/hit_df['mean_number'].sum(),3)))\n",
    "    # print(hit_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    The function shows the 5' and 3' base addition modifications of tRFs.\n",
    "    The function will print the top 5' and 3' addition modifications\n",
    "    And draw four types/columns of pie charts:\n",
    "        1. Pie charts showing the ratio of tRFs with or without 5'-Addition modification\n",
    "        2. Pie charts showing the composition of 5'-Addition modifications\n",
    "        3. Pie charts showing the ratio of tRFs with or without 3'-Addition modification\n",
    "        4. Pie charts showing the composition of 3'-Addition modifications\n",
    "    @param d: The data object generated by data_loader.py\n",
    "    @param proj_name: the name of project, the default is \"test\", use can check the name of hit.tab file\n",
    "            it follows the pattern : <sampleID>+\"_\"+<proj_name>+\"_hit.tab\"\n",
    "    @param top_num: The number of top addition modifications to be printed out\n",
    "    @param radius: the radius of pies\n",
    "    @param fontsize:\n",
    "    @param fig_width:\n",
    "    @return: None\n",
    "'''\n",
    "# rs.getAddModificationStatisitc(d, 'test')\n",
    "\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# pd.options.plotting.backend = \"plotly\"\n",
    "# \n",
    "# proj_name =\"test\"\n",
    "# sample_dic = d[\"sample_dic\"]\n",
    "# s_num =len(sample_dic.keys())\n",
    "# \n",
    "# sample_ls = list(sample_dic.keys())\n",
    "# sample_ls.sort()\n",
    "# \n",
    "# \n",
    "# fig, axs = plt.subplots(s_num, 4, figsize=[14, s_num*3+2])\n",
    "# radius = 1.2\n",
    "# fontsize =14\n",
    "# index = 0\n",
    "# for s in sample_ls:\n",
    "#     des = sample_dic[s]\n",
    "#     hit_tab = d['wdir']+\"/\"+s+\"_\"+proj_name+\"_hit.tab\"\n",
    "#     df = pd.read_csv(hit_tab, sep=\"\\t\")\n",
    "# \n",
    "#     df = dl.add_aa_column(df, trna_id=\"tRNA_id\")\n",
    "# \n",
    "#     df['temp5'] = df['read_5_fragment']\n",
    "#     group_a = df.groupby('read_5_fragment').sum()\n",
    "#     axs[index,1].pie(group_a[\"mean_number\"],labels=group_a.index,autopct='%1.1f%%',shadow=False, radius=radius,labeldistance=None)\n",
    "#     print(\"\\n\"+s+\"_\"+des)\n",
    "#     sorted_df =group_a.sort_values([\"mean_number\"], ascending=[False]).head(10)\n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also  \n",
    "#         print(sorted_df['mean_number'])\n",
    "#     group_c = df.groupby('read_3_fragment').sum()\n",
    "#     sorted_df =group_c.sort_values([\"mean_number\"], ascending=[False]).head(10)\n",
    "#     with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also  \n",
    "#         print(sorted_df['mean_number'])\n",
    "#     axs[index,3].pie(group_c[\"mean_number\"],labels=group_c.index,autopct='%1.1f%%',shadow=False, radius=radius, labeldistance=None)\n",
    "#     #axs[index,1].text(-0.06, 0.5, des, transform=axs[index,0].transAxes,va='center', fontsize=12, weight='bold', rotation=90)\n",
    "# \n",
    "#     df.loc[df['temp5'].isna(),'read_5_fragment']='-'\n",
    "#     df.loc[df['temp5'].notna(),'read_5_fragment']='Addition'\n",
    "#     group_b = df.groupby('read_5_fragment').sum()\n",
    "#     axs[index,0].pie(group_b[\"mean_number\"],labels=group_b.index,autopct='%1.1f%%',shadow=False, radius=radius)\n",
    "#     axs[index,0].text(-0.1, 0.5, des, transform=axs[index,0].transAxes,va='center', fontsize=fontsize, weight='bold', rotation=90)\n",
    "# \n",
    "#     df['temp3'] = df['read_3_fragment']                                                     \n",
    "#     df.loc[df['temp3'].isna(),'read_3_fragment']='-'\n",
    "#     df.loc[df['temp3'].notna(),'read_3_fragment']='Addition'\n",
    "#     group = df.groupby('read_3_fragment').sum()\n",
    "#     axs[index,2].pie(group[\"mean_number\"],labels=group.index,autopct='%1.1f%%',shadow=False, radius=radius)\n",
    "#     if index==0:\n",
    "#         axs[index,0].text(0.2, 1.1, \"5'_Addition_Ratio\", transform=axs[index,0].transAxes,va='center', fontsize=fontsize, weight='bold', rotation=0)\n",
    "#         axs[index,1].text(1.4, 1.1, \"5'_Addition_Dist\", transform=axs[index,0].transAxes,va='center', fontsize=fontsize, weight='bold', rotation=0)\n",
    "#         axs[index,2].text(2.6, 1.1, \"3'_Addition_Ratio\", transform=axs[index,0].transAxes,va='center', fontsize=fontsize, weight='bold', rotation=0)\n",
    "#         axs[index,3].text(3.9, 1.1, \"3'_Addition_Dist\", transform=axs[index,0].transAxes,va='center', fontsize=fontsize, weight='bold', rotation=0)\n",
    "#     index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    The function shows the 5' and 3' base addition modifications of tRFs.\n",
    "    The function will print the top 5' and 3' addition modifications\n",
    "    And draw four types/columns of pie charts:\n",
    "        1. Pie charts showing the ratio of tRFs with or without 5'-Addition modification\n",
    "        2. Pie charts showing the composition of 5'-Addition modifications\n",
    "        3. Pie charts showing the ratio of tRFs with or without 3'-Addition modification\n",
    "        4. Pie charts showing the composition of 3'-Addition modifications\n",
    "    @param d: The data object generated by data_loader.py\n",
    "    @param proj_name: the name of project, the default is \"test\", use can check the name of hit.tab file\n",
    "            it follows the pattern : <sampleID>+\"_\"+<proj_name>+\"_hit.tab\"\n",
    "    @param top_num: The number of top addition modifications to be printed out\n",
    "    @param radius: the radius of pies\n",
    "    @param fontsize: font size of the title \n",
    "    @param fig_width: the width of the figure\n",
    "    @param fig_height: the height of the figure\n",
    "    @return: None\n",
    "'''\n",
    "rs.getAddModificationStatisitc2(d, \n",
    "                                proj_name = 'test', \n",
    "                                top_num=10,\n",
    "                                fontsize=12,\n",
    "                                fig_width=600,\n",
    "                                fig_height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "import re\n",
    "#print(df.columns)\n",
    "\n",
    "a=\"GCAACGTCACGCGGGAGCCAAAAAAAAAA\"\n",
    "i ='NNNNCG'\n",
    "p = \"^\"+i.replace('N','\\w')\n",
    "\n",
    "\",aaa\"\n",
    "print(\",aas\".split(','))\n",
    "print(p)\n",
    "#p = '\\w\\w\\w\\wCG'\n",
    "b = re.search(p, a)\n",
    "if b: \n",
    "    print(b.group(0))\n",
    "    print(b.span()[0])\n",
    "    print (a[b.span()[1]:])\n",
    "\n",
    "r_patterm=\"AAAAAAAAA\"\n",
    "p = \"CCA\"+r_patterm.replace('N','\\w')\n",
    "b = re.search(p, a)\n",
    "if b:\n",
    "    seq =a[:b.span()[0]+3]\n",
    "    print(seq)\n",
    "\n",
    "dict1 = OrderedDict(sorted(d[\"sample_dic\"].items())) \n",
    "# print(dict1)\n",
    "top_num = 5\n",
    "for s, s_des in dict1.items():\n",
    "    tab_file=wdir+\"/\"+s+\"_test_hit.tab\"\n",
    "    df = pd.read_csv(tab_file, sep=\"\\t\", index_col=False).fillna('')\n",
    "    print(df.head(10)[['read_5_fragment','read_fragment']])\n",
    "    \n",
    "    \n",
    "    hit_df=dl.add_aa_column(df, trna_id=\"tRNA_id\")\n",
    "    read_5_num = float(hit_df.loc[hit_df['read_5_fragment']!=\"\"]['mean_number'].sum())\n",
    "    total_read_num= float(hit_df['mean_number'].sum())\n",
    "    print(s_des+\":\"+s+\":\"+str(read_5_num/total_read_num))\n",
    "    \n",
    "    aa_group_df = aa_hit_df.groupby('read_5_fragment').sum()\n",
    "    amio_sorted_df = aa_group_df.sort_values([\"mean_number\"], ascending=[False]).head(top_num)\n",
    "    #print(amio_sorted_df['mean_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def filterFastQ2FastA(fastq, filtered_fasta, num_dic_txt, qcutoff=0, num_cutoff=50):\n",
    "    FASTQ  = open(fastq,'r')\n",
    "    iFASTQ = iter(FASTQ)\n",
    "    dic = {}\n",
    "    name_dic = {}\n",
    "    Filterecd_FASTA = open(filtered_fasta, 'w')\n",
    "    NumDicFile = open(num_dic_txt,\"w\")\n",
    "    total_num = 0\n",
    "    low_quality_num=0\n",
    "    high_quality_num = 0\n",
    "    for line in iFASTQ:\n",
    "        line  = line.strip()\n",
    "        if line.startswith('@'):\n",
    "            total_num+=1\n",
    "            title = line.replace(\" \",\"_\")\n",
    "            seq = next(iFASTQ).strip()\n",
    "            title2 = next(iFASTQ).strip()\n",
    "            quanlity = next(iFASTQ).strip()\n",
    "            low_quanlity = False\n",
    "            if total_num % 2000000==0:\n",
    "                print(\"reads (\"+str(total_num)+\")\")\n",
    "                print(\"dic (\"+str(len(dic.keys()))+\")\")\n",
    "                \n",
    "\n",
    "            # for i in quanlity:\n",
    "            #     if ord(i)<=qcutoff+33:  #Based on Phred 33\n",
    "            #         low_quanlity = True\n",
    "            #         break\n",
    "            if not low_quanlity:\n",
    "                high_quality_num+=1\n",
    "                if seq not in dic:\n",
    "                    dic[seq] = {\n",
    "                        \"title\":title,\n",
    "                        #\"seq\":seq,\n",
    "                        \"count\":1\n",
    "                    }\n",
    "                    name_dic[seq] = title\n",
    "                else:\n",
    "                    dic[seq][\"count\"]+=1\n",
    "            else:\n",
    "                low_quality_num+=1\n",
    "    FASTQ.close()\n",
    "    non_redundent_num = 0\n",
    "    for seq in dic:\n",
    "        title = name_dic[seq]\n",
    "        if dic[seq][\"count\"]>=num_cutoff:\n",
    "            Filterecd_FASTA.write(\">\" + dic[seq][\"title\"] + \"\\n\")\n",
    "            Filterecd_FASTA.write(seq + \"\\n\")\n",
    "            non_redundent_num += 1\n",
    "            NumDicFile.write(title+\"\\t\"+str(dic[seq][\"count\"])+\"\\t\"+seq+\"\\n\")\n",
    "    NumDicFile.close()\n",
    "    Filterecd_FASTA.close()\n",
    "    return({\"total_num\":int(total_num),\n",
    "          \"removed_num\":int(low_quality_num),\n",
    "          \"survived_num\":int(high_quality_num),\n",
    "          \"non_redundent_num\":int(non_redundent_num)})\n",
    "# get starting time\n",
    "start = time.time()\n",
    "\n",
    "fastq = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495.fastq\"\n",
    "filtered_fasta = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495_f.fastq\"\n",
    "num_dic_txt = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495.dic.txt\"\n",
    "print(filterFastQ2FastA(fastq,filtered_fasta,num_dic_txt))\n",
    "\n",
    "elapsed_time_fl = (time.time() - start) \n",
    "print(elapsed_time_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# processing the AA tail\n",
    "from collections import OrderedDict \n",
    "import re\n",
    "\n",
    "\n",
    "dict1 = OrderedDict(sorted(d[\"sample_dic\"].items())) \n",
    "# print(dict1)\n",
    "\n",
    "s_df = pd.DataFrame([], columns=['id','des','aa_ratio','cca_aa_reads','aa_reads','noaa_reads','total_reads'])\n",
    "\n",
    "for s, s_des in dict1.items():\n",
    "    tab_file=wdir+\"/\"+s+\"_test_hit.tab\"\n",
    "    df = pd.read_csv(tab_file, sep=\"\\t\", index_col=False).fillna('')\n",
    "    \n",
    "    hit_df=dl.add_aa_column(df, trna_id=\"tRNA_id\")\n",
    "    hit_df=dl.add_AA_statistic(hit_df, column=\"read_3_fragment\")\n",
    "    #print(hit_df.columns)\n",
    "    \n",
    "    top_num=10\n",
    "    hit_df=hit_df[hit_df['AA_tail']!=\"\"][hit_df['AA_length']>7] #[hit_df['TRF_type']!=\"full_tRNA\"]\n",
    "    group_a = hit_df.groupby('AA_length').sum()\n",
    "    sorted_df = group_a.sort_values([\"mean_number\"], ascending=[False]).head(top_num)\n",
    "    print(sorted_df[\"mean_number\"])\n",
    "    \n",
    "    # .str.contains('Ju')\n",
    "    aa_hit_df = hit_df[hit_df['read_3_fragment'].str.contains('AAAAAAA')]\n",
    "    aa_group_df = aa_hit_df.groupby('TRF_type').sum()\n",
    "    \n",
    "    aa_amino_group_df = aa_hit_df.groupby('AA').sum()\n",
    "    amio_sorted_df = aa_amino_group_df.sort_values([\"mean_number\"], ascending=[False]).head(top_num)\n",
    "    print(amio_sorted_df[\"mean_number\"])\n",
    "    \n",
    "    cca_aa_hit_df = aa_hit_df[aa_hit_df['read_fragment'].str.endswith('CCA')]\n",
    "    #print(aa_hit_df['read_3_fragment'])\n",
    "    cca_aa_reads = cca_aa_hit_df['mean_number'].sum()\n",
    "    aa_reads = aa_hit_df['mean_number'].sum()\n",
    "    noaa_hit_df = hit_df[~hit_df['read_3_fragment'].str.contains('AAAAAAA')]\n",
    "    noaa_reads = noaa_hit_df['mean_number'].sum()\n",
    "    print(\"######\"+s_des)\n",
    "    print(aa_group_df['mean_number'])\n",
    "    print(str(int(cca_aa_reads))+\":\"+str(int(aa_reads))+\":\"+str(int(noaa_reads))+\":\"+str(int(aa_reads+noaa_reads)))\n",
    "    total_reads = aa_reads+noaa_reads\n",
    "    aa_ratio = round(aa_reads/hit_df['mean_number'].sum(),3)\n",
    "    print(s_des+\":\"+str(round(aa_ratio,3)))\n",
    "    s_df = s_df.append({\n",
    "        'id':s,\n",
    "        'des':s_des,\n",
    "        'cca_aa_reads':int(cca_aa_reads),\n",
    "        'aa_reads':int(aa_reads),\n",
    "        'noaa_reads':int(noaa_reads),\n",
    "        'total_reads':int(total_reads),\n",
    "        'aa_ratio': aa_ratio\n",
    "    }, ignore_index=True)\n",
    "print(s_df)\n",
    "    # print(hit_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gc\n",
    "import os\n",
    "def filterFastQ2FastA(fastq, filtered_fasta, num_dic_txt, qcutoff=0, num_cutoff=50):\n",
    "    big_file = False\n",
    "    if os.path.getsize(fastq)>3259059200:\n",
    "        big_file=True\n",
    "    FASTQ  = open(fastq,'r')\n",
    "    Filterecd_FASTA = open(filtered_fasta, 'w')\n",
    "    NumDicFile = open(num_dic_txt,\"w\")\n",
    "    dic = {}\n",
    "    name_dic = {}\n",
    "    total_num = 0\n",
    "    low_quality_num=0\n",
    "    high_quality_num = 0\n",
    "    line_index=0\n",
    "    seq=\"\"\n",
    "    title=\"\"\n",
    "    title2=\"\"\n",
    "    quanlity=\"\"\n",
    "    for line in FASTQ:\n",
    "        line  = line.strip()\n",
    "        line_index+=1\n",
    "        if line_index%4==2:\n",
    "            seq = line\n",
    "        elif line_index%4==3:\n",
    "            continue\n",
    "            # title2 = line.replace(\" \",\"_\")\n",
    "        elif line_index%4==0:\n",
    "            quanlity = line\n",
    "        elif line.startswith('@') and line_index%4==1 and seq!=\"\":\n",
    "            low_quanlity = False\n",
    "            title=line.replace(\" \",\"_\")\n",
    "            if total_num % 400000==0:\n",
    "                print(\"reads (\"+str(total_num)+\")\")\n",
    "                keys = list(dic.keys())\n",
    "                if big_file:\n",
    "                    for k in keys:\n",
    "                        if (dic[k][\"count\"]==1):\n",
    "                            del dic[k]\n",
    "                print(\"dic (\"+str(len(dic.keys()))+\")\")\n",
    "                gc.collect()\n",
    "            # for i in quanlity:\n",
    "            #     if ord(i)<=qcutoff+33:  #Based on Phred 33\n",
    "            #         low_quanlity = True\n",
    "            #         break\n",
    "            if not low_quanlity :\n",
    "                high_quality_num+=1\n",
    "                if seq not in dic:\n",
    "                    dic[seq] = {\n",
    "                        \"title\":title,\n",
    "                        #\"seq\":seq,\n",
    "                        \"count\":1\n",
    "                    }\n",
    "                    name_dic[seq] = title\n",
    "                else:\n",
    "                    dic[seq][\"count\"]+=1\n",
    "            total_num+=1\n",
    "    FASTQ.close()\n",
    "    non_redundent_num = 0\n",
    "    for seq in dic:\n",
    "        title = name_dic[seq]\n",
    "        if dic[seq][\"count\"]>=num_cutoff:\n",
    "            Filterecd_FASTA.write(\">\" + dic[seq][\"title\"] + \"\\n\")\n",
    "            Filterecd_FASTA.write(seq + \"\\n\")\n",
    "            non_redundent_num += 1\n",
    "            NumDicFile.write(title+\"\\t\"+str(dic[seq][\"count\"])+\"\\t\"+seq+\"\\n\")\n",
    "    NumDicFile.close()\n",
    "    Filterecd_FASTA.close()\n",
    "    return({\"total_num\":int(total_num),\n",
    "          \"removed_num\":int(total_num-high_quality_num),\n",
    "          \"survived_num\":int(high_quality_num),\n",
    "          \"non_redundent_num\":int(non_redundent_num)})\n",
    "# get starting time\n",
    "start = time.time()\n",
    "\n",
    "fastq = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495.fastq\"\n",
    "filtered_fasta = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495_f.fastq\"\n",
    "num_dic_txt = \"/Users/hqyone/GitHub/rep_project/data/ngs_datasource/RNA_Seq_test/fastq/TestData/WT_1/ERX424842/ERR458495.dic.txt\"\n",
    "print(filterFastQ2FastA(fastq,filtered_fasta,num_dic_txt))\n",
    "\n",
    "elapsed_time_fl = (time.time() - start) \n",
    "print(elapsed_time_fl)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
